# Copyright The OpenTelemetry Authors
# SPDX-License-Identifier: Apache-2.0

version: '3.9'
x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"

services:
  # ******************
  # Core Demo Services
  # ******************
  # Accounting service
  accountingservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-accountingservice
    container_name: accounting-service
    build:
      context: ./
      dockerfile: ./src/accountingservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-accountingservice
    deploy:
      resources:
        limits:
          memory: 20M
    restart: unless-stopped
    environment:
      - KAFKA_SERVICE_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=accountingservice
    depends_on:
      otelcol:
        condition: service_started
      kafka:
        condition: service_healthy
    logging: *logging

  # AdService
  adservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-adservice
    container_name: ad-service
    build:
      context: ./
      dockerfile: ./src/adservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-adservice
    deploy:
      resources:
        limits:
          memory: 300M
    restart: unless-stopped
    ports:
      - "${AD_SERVICE_PORT}"
    environment:
      - AD_SERVICE_PORT
      - FEATURE_FLAG_GRPC_SERVICE_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_LOGS_EXPORTER=otlp
      - OTEL_SERVICE_NAME=adservice
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging

  # Cart service
  cartservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-cartservice
    container_name: cart-service
    build:
      context: ./
      dockerfile: ./src/cartservice/src/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-cartservice
    deploy:
      resources:
        limits:
          memory: 160M
    restart: unless-stopped
    ports:
      - "${CART_SERVICE_PORT}"
    environment:
      - CART_SERVICE_PORT
      - REDIS_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=cartservice
      - ASPNETCORE_URLS=http://*:${CART_SERVICE_PORT}
    depends_on:
      redis-cart:
        condition: service_started
      otelcol:
        condition: service_started
    logging: *logging

  # Checkout service
  checkoutservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-checkoutservice
    container_name: checkout-service
    build:
      context: ./
      dockerfile: ./src/checkoutservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-checkoutservice
    deploy:
      resources:
        limits:
          memory: 20M
    restart: unless-stopped
    ports:
      - "${CHECKOUT_SERVICE_PORT}"
    environment:
      - CHECKOUT_SERVICE_PORT
      - CART_SERVICE_ADDR
      - CURRENCY_SERVICE_ADDR
      - EMAIL_SERVICE_ADDR
      - PAYMENT_SERVICE_ADDR
      - PRODUCT_CATALOG_SERVICE_ADDR
      - SHIPPING_SERVICE_ADDR
      - KAFKA_SERVICE_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=checkoutservice
    depends_on:
      cartservice:
        condition: service_started
      currencyservice:
        condition: service_started
      emailservice:
        condition: service_started
      paymentservice:
        condition: service_started
      productcatalogservice:
        condition: service_started
      shippingservice:
        condition: service_started
      otelcol:
        condition: service_started
      kafka:
        condition: service_healthy
    logging: *logging

  # Currency service
  currencyservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-currencyservice
    container_name: currency-service
    build:
      context: ./src/currencyservice
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-currencyservice
      args:
        - GRPC_VERSION=1.46.0
        - OPENTELEMETRY_VERSION=1.5.0
    deploy:
      resources:
        limits:
          memory: 20M
    restart: unless-stopped
    ports:
      - "${CURRENCY_SERVICE_PORT}"
    environment:
      - CURRENCY_SERVICE_PORT
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES},service.name=currencyservice   # The C++ SDK does not support OTEL_SERVICE_NAME
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging

  # Email service
  emailservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-emailservice
    container_name: email-service
    build:
      context: ./src/emailservice
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-emailservice
    deploy:
      resources:
        limits:
          memory: 100M
    restart: unless-stopped
    ports:
      - "${EMAIL_SERVICE_PORT}"
    environment:
      - APP_ENV=production
      - EMAIL_SERVICE_PORT
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://${OTEL_COLLECTOR_HOST}:4318/v1/traces
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=emailservice
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging

  # Feature Flag service
  featureflagservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-featureflagservice
    container_name: feature-flag-service
    build:
      context: ./
      dockerfile: ./src/featureflagservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-featureflagservice
    deploy:
      resources:
        limits:
          memory: 175M
    restart: unless-stopped
    ports:
      - "${FEATURE_FLAG_SERVICE_PORT}"                                  # Feature Flag Service UI
      - "${FEATURE_FLAG_GRPC_SERVICE_PORT}"                             # Feature Flag Service gRPC API
    environment:
      - FEATURE_FLAG_SERVICE_PORT
      - FEATURE_FLAG_GRPC_SERVICE_PORT
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=grpc
      - OTEL_SERVICE_NAME=featureflagservice
      - DATABASE_URL=ecto://ffs:ffs@ffs_postgres:5432/ffs
    healthcheck:
      test: ["CMD", "curl", "-H", "baggage: synthetic_request=true", "-f", "http://localhost:${FEATURE_FLAG_SERVICE_PORT}"]
    depends_on:
      ffs_postgres:
        condition: service_healthy
    logging: *logging

  # Fraud Detection service
  frauddetectionservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-frauddetectionservice
    container_name: frauddetection-service
    build:
      context: ./
      dockerfile: ./src/frauddetectionservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-frauddetectionservice
    deploy:
      resources:
        limits:
          memory: 200M
    restart: unless-stopped
    environment:
      - KAFKA_SERVICE_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=frauddetectionservice
    depends_on:
      otelcol:
        condition: service_started
      kafka:
        condition: service_healthy
    logging: *logging

  # Frontend
  frontend:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-frontend
    container_name: frontend
    build:
      context: ./
      dockerfile: ./src/frontend/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-frontend
    deploy:
      resources:
        limits:
          memory: 200M
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT}"
    environment:
      - PORT=${FRONTEND_PORT}
      - FRONTEND_ADDR
      - AD_SERVICE_ADDR
      - CART_SERVICE_ADDR
      - CHECKOUT_SERVICE_ADDR
      - CURRENCY_SERVICE_ADDR
      - PRODUCT_CATALOG_SERVICE_ADDR
      - RECOMMENDATION_SERVICE_ADDR
      - SHIPPING_SERVICE_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_RESOURCE_ATTRIBUTES=${OTEL_RESOURCE_ATTRIBUTES}
      - ENV_PLATFORM
      - OTEL_SERVICE_NAME=frontend
      - PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - WEB_OTEL_SERVICE_NAME=frontend-web
    depends_on:
      adservice:
        condition: service_started
      cartservice:
        condition: service_started
      checkoutservice:
        condition: service_started
      currencyservice:
        condition: service_started
      productcatalogservice:
        condition: service_started
      quoteservice:
        condition: service_started
      recommendationservice:
        condition: service_started
      shippingservice:
        condition: service_started
      otelcol:
        condition: service_started
    logging: *logging

  # Frontend Proxy (Envoy)
  frontendproxy:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-frontendproxy
    container_name: frontend-proxy
    build:
      context: ./
      dockerfile: src/frontendproxy/Dockerfile
    deploy:
      resources:
        limits:
          memory: 50M
    ports:
      - "${ENVOY_PORT}:${ENVOY_PORT}"
      - 10000:10000
    environment:
      - FRONTEND_PORT
      - FRONTEND_HOST
      - FEATURE_FLAG_SERVICE_PORT
      - FEATURE_FLAG_SERVICE_HOST
      - LOCUST_WEB_HOST
      - LOCUST_WEB_PORT
      - GRAFANA_SERVICE_PORT
      - GRAFANA_SERVICE_HOST
      - JAEGER_SERVICE_PORT
      - JAEGER_SERVICE_HOST
      - OTEL_COLLECTOR_HOST
      - OTEL_COLLECTOR_PORT_GRPC
      - OTEL_COLLECTOR_PORT_HTTP
      - ENVOY_PORT
    depends_on:
      frontend:
        condition: service_started
      featureflagservice:
        condition: service_started
      loadgenerator:
        condition: service_started
      jaeger:
        condition: service_started
      grafana:
        condition: service_started

  # Load Generator
  loadgenerator:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-loadgenerator
    container_name: load-generator
    build:
      context: ./
      dockerfile: ./src/loadgenerator/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-loadgenerator
    deploy:
      resources:
        limits:
          memory: 120M
    restart: unless-stopped
    ports:
      - "${LOCUST_WEB_PORT}"
    environment:
      - LOCUST_WEB_PORT
      - LOCUST_USERS
      - LOCUST_HOST
      - LOCUST_HEADLESS
      - LOCUST_AUTOSTART
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=loadgenerator
      - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
    depends_on:
      frontend:
        condition: service_started
    logging: *logging

  # Payment service
  paymentservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-paymentservice
    container_name: payment-service
    build:
      context: ./
      dockerfile: ./src/paymentservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-paymentservice
    deploy:
      resources:
        limits:
          memory: 120M
    restart: unless-stopped
    ports:
      - "${PAYMENT_SERVICE_PORT}"
    environment:
      - PAYMENT_SERVICE_PORT
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=paymentservice
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging

  # Product Catalog service
  productcatalogservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-productcatalogservice
    container_name: product-catalog-service
    build:
      context: ./
      dockerfile: ./src/productcatalogservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-productcatalogservice
    deploy:
      resources:
        limits:
          memory: 20M
    restart: unless-stopped
    ports:
      - "${PRODUCT_CATALOG_SERVICE_PORT}"
    environment:
      - PRODUCT_CATALOG_SERVICE_PORT
      - FEATURE_FLAG_GRPC_SERVICE_ADDR
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=productcatalogservice
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging

  # Quote service
  quoteservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-quoteservice
    container_name: quote-service
    build:
      context: ./
      dockerfile: ./src/quoteservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-quoteservice
    deploy:
      resources:
        limits:
          memory: 40M
    restart: unless-stopped
    ports:
      - "${QUOTE_SERVICE_PORT}"
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://${OTEL_COLLECTOR_HOST}:4318
      - OTEL_PHP_AUTOLOAD_ENABLED=true
      - QUOTE_SERVICE_PORT
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=quoteservice
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging

  # Recommendation service
  recommendationservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-recommendationservice
    container_name: recommendation-service
    build:
      context: ./
      dockerfile: ./src/recommendationservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-recommendationservice
    deploy:
      resources:
        limits:
          memory: 500M               # This is high to enable supporting the recommendationCache feature flag use case
    restart: unless-stopped
    ports:
      - "${RECOMMENDATION_SERVICE_PORT}"
    environment:
      - RECOMMENDATION_SERVICE_PORT
      - PRODUCT_CATALOG_SERVICE_ADDR
      - FEATURE_FLAG_GRPC_SERVICE_ADDR
      - OTEL_PYTHON_LOG_CORRELATION=true
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=recommendationservice
      - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
    depends_on:
      featureflagservice:
        condition: service_started
      productcatalogservice:
        condition: service_started
      otelcol:
        condition: service_started
    logging: *logging

  # Shipping service
  shippingservice:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-shippingservice
    container_name: shipping-service
    build:
      context: ./
      dockerfile: ./src/shippingservice/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-shippingservice
    deploy:
      resources:
        limits:
          memory: 20M
    restart: unless-stopped
    ports:
      - "${SHIPPING_SERVICE_PORT}"
    environment:
      - SHIPPING_SERVICE_PORT
      - QUOTE_SERVICE_ADDR
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://${OTEL_COLLECTOR_HOST}:4317/v1/traces
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=shippingservice
    depends_on:
      otelcol:
        condition: service_started
    logging: *logging

  # ******************
  # Dependent Services
  # ******************
  # Postgres used by Feature Flag service
  ffs_postgres:
    image: postgres:14
    container_name: postgres
    user: postgres
    deploy:
      resources:
        limits:
          memory: 120M
    restart: unless-stopped
    environment:
      - POSTGRES_USER=ffs
      - POSTGRES_DB=ffs
      - POSTGRES_PASSWORD=ffs
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d ffs -U ffs"]
      interval: 10s
      timeout: 5s
      retries: 5
    logging: *logging

  # Kafka used by Checkout, Accounting, and Fraud Detection services
  kafka:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-kafka
    container_name: kafka
    build:
      context: ./
      dockerfile: ./src/kafka/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-kafka
    deploy:
      resources:
        limits:
          memory: 500M
    restart: unless-stopped
    environment:
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - OTEL_EXPORTER_OTLP_ENDPOINT
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
      - OTEL_RESOURCE_ATTRIBUTES
      - OTEL_SERVICE_NAME=kafka
      - KAFKA_HEAP_OPTS=-Xmx200m -Xms200m
    healthcheck:
      test: nc -z kafka 9092
      start_period: 10s
      interval: 5s
      timeout: 10s
      retries: 10
    logging: *logging

  # Redis used by Cart service
  redis-cart:
    image: redis:alpine
    container_name: redis-cart
    user: redis
    deploy:
      resources:
        limits:
          memory: 20M
    restart: unless-stopped
    ports:
      - "${REDIS_PORT}"
    logging: *logging


  # ********************
  # Telemetry Components
  # ********************
  # Jaeger
  jaeger:
    image: jaegertracing/all-in-one
    container_name: jaeger
    command:
      - "--memory.max-traces"
      - "10000"
      - "--query.base-path"
      - "/jaeger/ui"
      - "--prometheus.server-url"
      - "http://${PROMETHEUS_ADDR}"
    deploy:
      resources:
        limits:
          memory: 300M
    restart: unless-stopped
    ports:
      - "${JAEGER_SERVICE_PORT}"                    # Jaeger UI
      - "4317"                           # OTLP gRPC default port
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
    logging: *logging

  # Grafana
  grafana:
    image: grafana/grafana:9.4.7
    container_name: grafana
    deploy:
      resources:
        limits:
          memory: 100M
    volumes:
      - ./src/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./src/grafana/provisioning/:/etc/grafana/provisioning/
    ports:
      - "${GRAFANA_SERVICE_PORT}"
    logging: *logging

  # OpenTelemetry Collector
  otelcol:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-otelcol
    container_name: otel-col
    build:
      context: ./
      dockerfile: ./src/otelcollector/Dockerfile
      cache_from:
        - ${IMAGE_NAME}:${IMAGE_VERSION}-otelcol
    deploy:
      resources:
        limits:
          memory: 125M
    restart: unless-stopped
    command: [ "--config=/etc/otelcol-config.yml", "--config=/etc/otelcol-observability.yml", "--config=/etc/otelcol-config-extras.yml" ]
    volumes:
      - ./src/otelcollector/otelcol-config.yml:/etc/otelcol-config.yml
      - ./src/otelcollector/otelcol-observability.yml:/etc/otelcol-observability.yml
      - ./src/otelcollector/otelcol-config-extras.yml:/etc/otelcol-config-extras.yml
    ports:
      - "4317"          # OTLP over gRPC receiver
      - "4318:4318"     # OTLP over HTTP receiver
      - "9464"          # Prometheus exporter
      - "8888"          # metrics endpoint
    depends_on:
      - jaeger
    logging: *logging

  # Prometheus
  prometheus:
    image: quay.io/prometheus/prometheus:v2.43.0
    container_name: prometheus
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --enable-feature=exemplar-storage
    volumes:
      - ./src/prometheus/prometheus-config.yaml:/etc/prometheus/prometheus-config.yaml
    deploy:
      resources:
        limits:
          memory: 300M
    ports:
      - "${PROMETHEUS_SERVICE_PORT}:${PROMETHEUS_SERVICE_PORT}"
    logging: *logging

  # *****
  # Tests
  # *****
  # Frontend Tests
  frontendTests:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-frontend-tests
    container_name: frontend-tests
    build:
      context: ./
      dockerfile: ./src/frontend/Dockerfile.cypress
    profiles:
      - tests
    volumes:
      - ./src/frontend/cypress/videos:/app/cypress/videos
      - ./src/frontend/cypress/screenshots:/app/cypress/screenshots
    environment:
      - CYPRESS_baseUrl=http://${FRONTEND_ADDR}
      - FRONTEND_ADDR
      - NODE_ENV=production
    depends_on:
      - frontend

  # Integration Tests
  integrationTests:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-integrationTests
    container_name: integrationTests
    profiles:
      - tests
    build:
      context: ./
      dockerfile: ./test/Dockerfile
    environment:
      - AD_SERVICE_ADDR
      - CART_SERVICE_ADDR
      - CHECKOUT_SERVICE_ADDR
      - CURRENCY_SERVICE_ADDR
      - EMAIL_SERVICE_ADDR
      - PAYMENT_SERVICE_ADDR
      - PRODUCT_CATALOG_SERVICE_ADDR
      - RECOMMENDATION_SERVICE_ADDR
      - SHIPPING_SERVICE_ADDR
    depends_on:
      - adservice
      - cartservice
      - checkoutservice
      - currencyservice
      - emailservice
      - paymentservice
      - productcatalogservice
      - recommendationservice
      - shippingservice
      - quoteservice

  # Tracebased Tests
  traceBasedTests:
    image: ${IMAGE_NAME}:${IMAGE_VERSION}-traceBasedTests
    container_name: traceBasedTests
    profiles:
      - tests
    build:
      context: ./
      dockerfile: ./test/tracetesting/Dockerfile
    environment:
      - AD_SERVICE_ADDR
      - CART_SERVICE_ADDR
      - CHECKOUT_SERVICE_ADDR
      - CURRENCY_SERVICE_ADDR
      - EMAIL_SERVICE_ADDR
      - FRONTEND_ADDR
      - PAYMENT_SERVICE_ADDR
      - PRODUCT_CATALOG_SERVICE_ADDR
      - RECOMMENDATION_SERVICE_ADDR
      - SHIPPING_SERVICE_ADDR
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      tracetest-server:
        condition: service_healthy
      # adding demo services as dependencies
      frontend:
        condition: service_started
      adservice:
        condition: service_started
      cartservice:
        condition: service_started
      checkoutservice:
        condition: service_started
      currencyservice:
        condition: service_started
      emailservice:
        condition: service_started
      paymentservice:
        condition: service_started
      productcatalogservice:
        condition: service_started
      recommendationservice:
        condition: service_started
      shippingservice:
        condition: service_started
      quoteservice:
        condition: service_started

  tracetest-server:
    image: kubeshop/tracetest:latest
    platform: linux/amd64
    container_name: tracetest-server
    profiles:
      - tests
    volumes:
      - type: bind
        source: ./test/tracetesting/tracetest-config.yaml
        target: /app/tracetest.yaml
      - type: bind
        source: ./test/tracetesting/tracetest-provision.yaml
        target: /app/provision.yaml
    command: --provisioning-file /app/provision.yaml
    ports:
      - 11633:11633
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      tracetest-postgres:
        condition: service_healthy
      otelcol:
        condition: service_started
    healthcheck:
      test: [ "CMD", "wget", "--spider", "localhost:11633" ]
      interval: 1s
      timeout: 3s
      retries: 60

  tracetest-postgres:
    image: postgres:14
    container_name: tracetest-postgres
    profiles:
      - tests
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
    healthcheck:
      test: pg_isready -U "$$POSTGRES_USER" -d "$$POSTGRES_DB"
      interval: 1s
      timeout: 5s
      retries: 60
    ports:
      - 5432

# Elastic stack
#
#

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTICSEARCH_VERSION}
    environment:
      - bootstrap.memory_lock=true
      - node.name=es01
      - discovery.type=single-node
        #- ELASTIC_PASSWORD=$ELASTIC_PASSWORD
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.license.self_generated.type=trial
      - xpack.security.enabled=true
      - xpack.security.authc.api_key.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=$CERTS_DIR/es01/es01.key
      - xpack.security.http.ssl.certificate_authorities=$CERTS_DIR/ca/ca.crt
      - xpack.security.http.ssl.certificate=$CERTS_DIR/es01/es01.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.certificate_authorities=$CERTS_DIR/ca/ca.crt
      - xpack.security.transport.ssl.certificate=$CERTS_DIR/es01/es01.crt
      - xpack.security.transport.ssl.key=$CERTS_DIR/es01/es01.key
        #- xpack.security.enrollment.enabled=true
    ulimits:
      memlock:
        hard: -1
        soft: -1
    volumes:
    - ./certs:/usr/share/elasticsearch/config/certs
    - ./esdata:/usr/share/elasticsearch/data
    ports:
    - 9200:9200
    networks:
    - opentelemetry-demo
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'

  apm-server:
    image: docker.elastic.co/apm/apm-server:${ELASTICSEARCH_VERSION}
    depends_on:
      - es01
      - kibana
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    networks:
    - opentelemetry-demo
    command: >
       apm-server -e
         -E apm-server.rum.enabled=true
         -E setup.kibana.host=kibana:5601
         -E setup.template.settings.index.number_of_replicas=0
         -E apm-server.kibana.enabled=true
         -E apm-server.kibana.host=kibana:5601
         -E output.elasticsearch.hosts=["es01:9200"]
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  apm-server-secure:
    image: docker.elastic.co/apm/apm-server:${ELASTICSEARCH_VERSION}
    depends_on:
      - es01
      - kibana
    cap_add: ["CHOWN", "DAC_OVERRIDE", "SETGID", "SETUID"]
    cap_drop: ["ALL"]
    ports:
      - 8200:8200
    networks:
      - opentelemetry-demo
    command: >
      apm-server -e
        -E apm-server.rum.enabled=true
        -E setup.kibana.host=kibana:5601
        -E setup.template.settings.index.number_of_replicas=0
        -E apm-server.kibana.enabled=true
        -E apm-server.kibana.host=kibana:5601
        -E output.elasticsearch.hosts=["es01:9200"]
        -E apm-server.auth.anonymous.rate_limit.event_limit=300
        -E apm-server.auth.anonymous.rate_limit.ip_limit=1000
        -E apm-server.auth.anonymous.allow_service=angular-app
        -E elasticsearch.password=${ELASTIC_PASSWORD}

    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  kibana:
    image: docker.elastic.co/kibana/kibana:${ELASTICSEARCH_VERSION}
    depends_on:
      - es01
    environment:
      - ELASTICSEARCH_HOSTS=https//es01:9200
        #- ELASTICSEARCH_USERNAME=kibana_system
        #- ELASTICSEARCH_PASSWORD="${ELASTIC_PASSWORD}"
        #- XPACK_FLEET_AGENTS_FLEET_SERVER_HOSTS="http://fleet-server:8220"
        #- XPACK_FLEET_AGENTS_ELASTICSEARCH_HOSTS="http://elasticsearch:9200"
      - XPACK_SECURITY_ENABLED=true
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
      - ELASTICSEARCH_SSL_VERIFICATIONMODE=certificate
      - SERVER_SSL_ENABLED=true
      - SERVER_SSL_KEY=config/certs/kibana/kibana.key
      - SERVER_SSL_CERTIFICATE=config/certs/kibana/kibana.crt
      #- xpack.monitoring.elasticsearch.ssl.verificationMode=certificate
    ports:
    - 5601:5601
    networks:
    - opentelemetry-demo
    volumes:
    - ./certs:/usr/share/kibana/config/certs
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status
    restart: on-failure

  fleet-server:
    image: docker.elastic.co/beats/elastic-agent:${ELASTICSEARCH_VERSION}
    ports:
    - 8220:8220
    healthcheck:
      test: ["CMD-SHELL", "curl -s -k https://localhost:8220/api/status | grep -q 'HEALTHY'"]
      retries: 300
      interval: 1s
    environment:
      - FLEET_SERVER_ENABLE="1"
      - FLEET_SERVER_ELASTICSEARCH_USERNAME="elastic"
      - FLEET_SERVER_ELASTICSEARCH_PASSWORD="${ELASTIC_PASSWORD}"
      - FLEET_SERVER_ES=http://es01:9200 
      - FLEET_SERVER_SERVICE_TOKEN=AAEAAWVsYXN0aWMvZmxlZXQtc2VydmVyL3Rva2VuLTE2OTA5OTk1ODcyODg6Rkgxa1Qybi1UdG13ejNzMHBFdHVfZw
      - FLEET_SERVER_POLICY=fleet-server-policy 
        #FLEET_SERVER_CERT: /etc/pki/tls/certs/fleet-server.pem
        #FLEET_SERVER_CERT_KEY: /etc/pki/tls/private/fleet-server-key.pem
        #- FLEET_URL="http://fleet-server:8220"
        #- KIBANA_FLEET_SETUP="true"
        #- KIBANA_FLEET_HOST="http://kibana:5601"
        #- KIBANA_FLEET_USERNAME="elastic"
        #- KIBANA_FLEET_PASSWORD="${ELASTIC_PASSWORD}"
        #depends_on:
        #- elasticsearch
        #- kibana
      #    volumes:
      #- "./testing/docker/fleet-server/certificate.pem:/etc/pki/tls/certs/fleet-server.pem"
      #- "./testing/docker/fleet-server/key.pem:/etc/pki/tls/private/fleet-server-key.pem"

  filebeat:
    image: docker.elastic.co/beats/filebeat:${ELASTICSEARCH_VERSION}
    user: root
    networks:
      - opentelemetry-demo
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/containers/:/var/lib/docker/containers/:ro
      - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
    environment:
      - ELASTICSEARCH_HOST=http://s01:9200
      - KIBANA_HOST=kibana
    command: ["--strict.perms=false"]
      #restart: on-failure

  metricbeat:
    image: docker.elastic.co/beats/metricbeat:${ELASTICSEARCH_VERSION}
    user: root
    networks:
      - opentelemetry-demo
    volumes:
      - /proc:/hostfs/proc:ro
      - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro
      - /:/hostfs:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./config/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro
    environment:
      - ELASTICSEARCH_HOST=http://es01:9200
      - KIBANA_HOST=kibana
    command: ["--strict.perms=false", "-system.hostfs=/hostfs"]
      #    restart: on-failure

volumes:
  esdata:
    driver: local
  certs:
    driver: local

networks:
  opentelemetry-demo:
    driver: bridge
